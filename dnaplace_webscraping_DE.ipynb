{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEKvEztdAPHpIRCmzhA4F+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdetras/dnaplace_DE_webscraping/blob/main/dnaplace_webscraping_DE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iQtP4WBBLO67"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "S6VLqWKHLTlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "#from google.colab import files\n",
        "\n",
        "# Upload your file\n",
        "#uploaded = files.upload()  # Upload cis_elements.txt\n",
        "#filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Read URLs from the uploaded file\n",
        "with open(filename, \"r\") as f:\n",
        "    urls = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Function to extract all DE lines from the preformatted text\n",
        "def extract_de(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        pre_tag = soup.find(\"pre\")\n",
        "        if not pre_tag:\n",
        "            return None\n",
        "\n",
        "        text_lines = pre_tag.get_text().splitlines()\n",
        "        de_lines = []\n",
        "        for line in text_lines:\n",
        "            if line.startswith(\"DE\"):\n",
        "                # Remove the 'DE' marker and leading whitespace\n",
        "                cleaned_line = line[2:].strip()\n",
        "                de_lines.append(cleaned_line)\n",
        "\n",
        "        if de_lines:\n",
        "            return \" \".join(de_lines)\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Extract DEs and store in a list\n",
        "data = []\n",
        "for url in urls:\n",
        "    de = extract_de(url)\n",
        "    data.append({\"URL\": url, \"DE\": de})\n",
        "\n",
        "# Write to CSV\n",
        "csv_filename = \"DE_extracted.csv\"\n",
        "with open(csv_filename, \"w\", newline='', encoding=\"utf-8\") as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=[\"URL\", \"DE\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(f\"CSV file '{csv_filename}' created successfully!\")\n",
        "\n",
        "# Auto-download CSV\n",
        "files.download(csv_filename)"
      ],
      "metadata": {
        "id": "ATaAjxdELV_i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}